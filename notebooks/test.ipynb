{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = './custom_LLM_final'\n",
    "device = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steelbear/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    fine_tuned_model,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"방청 페인트의 종류에는 어떤 것들이 있는지 알고 계신가요? 또한, 원목사이딩을 사용하는 것에 어떤 단점이 있을까요?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32094, 44881, 32712, 33402, 30708, 40880, 32221, 32886, 34859,\n",
       "         35351, 32907, 41934, 34415, 29973, 33169, 29892, 32133, 44994, 33716,\n",
       "         45372, 31286, 36796, 35840, 32886, 32212, 34652, 32687, 34069, 29973]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steelbear/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/steelbear/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/steelbear/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1413: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32094, 44881, 32712, 33402, 30708, 40880, 32221, 32886, 34859,\n",
       "         35351, 32907, 41934, 34415, 29973, 33169, 29892, 32133, 44994, 33716,\n",
       "         45372, 31286, 36796, 35840, 32886, 32212, 34652, 32687, 34069, 29973,\n",
       "         29871, 29896, 29889, 32094, 44881, 32712, 33402, 45001, 29973, 32094,\n",
       "         44881, 32712, 33402, 45001, 29892, 34131, 30393, 33568, 30811, 36971,\n",
       "         32166, 32712, 33402, 32214, 29889, 38201, 32578, 31973, 29892, 32160,\n",
       "         35520, 33806, 29892, 32118, 45025, 31362, 45971, 29892, 32008, 31285,\n",
       "         29892, 32135, 29892, 32034, 44906, 29892, 32839, 31000, 29892, 32839,\n",
       "         31000, 31980, 44830, 29892, 32160, 35520, 33806, 32160, 45701, 29892,\n",
       "         32160, 35520, 33806, 32160, 45701, 44930, 29892, 32160, 35520, 33806,\n",
       "         32160, 45701, 45164, 29892, 32160, 35520, 33806, 32160, 45701, 44837,\n",
       "         29892, 32160, 35520, 33806, 32160, 45701, 44858, 36500, 29892, 32160,\n",
       "         35520, 33806, 32160, 45701, 44930, 45852, 29892, 32160, 35520, 33806,\n",
       "         32160, 45701, 45043, 45219, 29892, 32160, 35520, 33806, 32160, 45701,\n",
       "         44503, 31950, 29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892,\n",
       "         32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806,\n",
       "         32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503,\n",
       "         29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520,\n",
       "         33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701,\n",
       "         44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160,\n",
       "         35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160,\n",
       "         45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892,\n",
       "         32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806,\n",
       "         32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503,\n",
       "         29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520,\n",
       "         33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701,\n",
       "         44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160,\n",
       "         35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160,\n",
       "         45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892,\n",
       "         32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806,\n",
       "         32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503,\n",
       "         29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520,\n",
       "         33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701,\n",
       "         44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160,\n",
       "         35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160,\n",
       "         45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892,\n",
       "         32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806,\n",
       "         32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503,\n",
       "         29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520,\n",
       "         33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701,\n",
       "         44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160,\n",
       "         35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160,\n",
       "         45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892,\n",
       "         32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806,\n",
       "         32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503,\n",
       "         29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520,\n",
       "         33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701,\n",
       "         44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160,\n",
       "         35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160,\n",
       "         45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892,\n",
       "         32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806,\n",
       "         32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503,\n",
       "         29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520,\n",
       "         33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701,\n",
       "         44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892, 32160,\n",
       "         35520, 33806, 32160, 45701, 44503, 29892, 32160, 35520, 33806, 32160,\n",
       "         45701, 44503, 29892, 32160, 35520, 33806, 32160, 45701, 44503, 29892,\n",
       "         32160, 35520]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=inputs.shape[1] + 512,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "outputs = model.generate(inputs, generation_config=generation_config)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> 방청 페인트의 종류에는 어떤 것들이 있는지 알고 계신가요? 또한, 원목사이딩을 사용하는 것에 어떤 단점이 있을까요? 1. 방청 페인트란? 방청 페인트란, 녹이 슬지 않도록 하는 페인트입니다. 주로 철재, 스테인리스, 알루미늄, 아연, 동, 주석, 황동, 황동합금, 스테인리스 스틸, 스테인리스 스틸판, 스테인리스 스틸봉, 스테인리스 스틸관, 스테인리스 스틸파이프, 스테인리스 스틸판넬, 스테인리스 스틸패널, 스테인리스 스틸커튼월, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인리스 스틸커튼, 스테인']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 572])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def test_data_generator():\n",
    "    with open('data/test.csv', 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader) # skip the header in csvfile\n",
    "        \n",
    "        for id, text in reader:\n",
    "            yield {'id': id, 'input': text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_dataset = Dataset.from_generator(test_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "\n",
    "prompter = Template(\"\"\"\n",
    "다음 질문을 읽고 적절한 답변을 작성하세요.\n",
    "#질문:${question}\n",
    "#답변:\"\"\")\n",
    "\n",
    "def tokenize(example):\n",
    "    prompt = prompter.substitute(question=example['input'])\n",
    "    tokenized = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    example['input_ids'] = tokenized\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'TEST_000',\n",
       " 'input': '방청 페인트의 종류에는 어떤 것들이 있는지 알고 계신가요? 또한, 원목사이딩을 사용하는 것에 어떤 단점이 있을까요?'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130/130 [00:00<00:00, 3589.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset.set_format(type=\"torch\")\n",
    "test_dataset = test_dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'TEST_000',\n",
       " 'input': '방청 페인트의 종류에는 어떤 것들이 있는지 알고 계신가요? 또한, 원목사이딩을 사용하는 것에 어떤 단점이 있을까요?',\n",
       " 'input_ids': tensor([[    1, 29871,    13, 37670, 32564, 34185, 42628, 43845, 35750, 31286,\n",
       "          36258, 33055, 29889,    13, 29937, 44913, 31406, 29901, 31945, 44881,\n",
       "          32712, 33402, 30708, 40880, 32221, 32886, 34859, 35351, 32907, 41934,\n",
       "          34415, 29973, 33169, 29892, 32133, 44994, 33716, 45372, 31286, 36796,\n",
       "          35840, 32886, 32212, 34652, 32687, 34069, 29973,    13, 29937, 45079,\n",
       "          45000, 29901]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/130 [00:00<?, ?it/s]/home/steelbear/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1413: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "  2%|▏         | 2/130 [10:05<10:45:59, 302.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m writer\u001b[38;5;241m.\u001b[39mwriterow([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m답변\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m tqdm(test_dataset):\n\u001b[0;32m---> 14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     output_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m     output_text \u001b[38;5;241m=\u001b[39m output_text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#답변:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1479\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1463\u001b[0m         input_ids,\n\u001b[1;32m   1464\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1476\u001b[0m     )\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1478\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2340\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2337\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2339\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2340\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2341\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2343\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2345\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2348\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1201\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1199\u001b[0m     logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1201\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1204\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/accelerate/hooks.py:160\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(old_forward)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 160\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/accelerate/hooks.py:286\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_map[name]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mint8:\n\u001b[1;32m    285\u001b[0m                 fp16_statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_map[name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCB\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m--> 286\u001b[0m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16_statistics\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(\n\u001b[1;32m    291\u001b[0m     kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[1;32m    292\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/accelerate/utils/modeling.py:298\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics)\u001b[0m\n\u001b[1;32m    296\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 298\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "            max_new_tokens=512,\n",
    "            )\n",
    "\n",
    "with open('inference.csv', 'w', newline='\\n') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', '답변'])\n",
    "\n",
    "    for example in tqdm(test_dataset):\n",
    "        outputs = model.generate(\n",
    "            example['input_ids'],\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "        output_text = tokenizer.batch_decode(outputs)[0]\n",
    "        output_text = output_text.split(\"#답변:\")[1]\n",
    "        writer.writerow([example['id'], output_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steelbear/.cache/pypoetry/virtualenvs/upstage-finetuning-pvueOKD_-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "modules.json: 100%|██████████| 341/341 [00:00<00:00, 2.67MB/s]\n",
      "config_sentence_transformers.json: 100%|██████████| 122/122 [00:00<00:00, 956kB/s]\n",
      "README.md: 100%|██████████| 2.45k/2.45k [00:00<00:00, 18.9MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 171kB/s]\n",
      "config.json: 100%|██████████| 556/556 [00:00<00:00, 3.11MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 539M/539M [07:05<00:00, 1.27MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 452/452 [00:00<00:00, 2.58MB/s]\n",
      "vocab.txt: 100%|██████████| 996k/996k [00:00<00:00, 1.84MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.96M/1.96M [00:01<00:00, 1.01MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 746kB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 1.19MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 1.58M/1.58M [00:00<00:00, 10.3MB/s]\n",
      "2_Dense/config.json: 100%|██████████| 114/114 [00:00<00:00, 833kB/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "embedding_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"방청 페인트의 종류에는 어떤 것들이 있는지 알고 계신가요? 또한, 원목사이딩을 사용하는 것에 어떤 단점이 있을까요?\"\n",
    "\n",
    "vector = embedding_model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07142449170351028,\n",
       " 0.029536643996834755,\n",
       " 0.022343140095472336,\n",
       " -0.0494682639837265,\n",
       " -0.016740785911679268,\n",
       " 0.011386487632989883,\n",
       " 0.028270971029996872,\n",
       " -0.010042344219982624,\n",
       " 0.023197857663035393,\n",
       " -0.01685791276395321,\n",
       " 0.010476252995431423,\n",
       " -0.032222505658864975,\n",
       " 0.05743233859539032,\n",
       " 0.004784977063536644,\n",
       " -0.06268604844808578,\n",
       " 0.009957283735275269,\n",
       " -0.027703575789928436,\n",
       " 0.04986656829714775,\n",
       " -0.062444865703582764,\n",
       " 0.03094945289194584,\n",
       " 0.016797414049506187,\n",
       " -0.05454891547560692,\n",
       " 0.010745197534561157,\n",
       " -0.015552802942693233,\n",
       " 0.03855337202548981,\n",
       " -0.0005605420446954668,\n",
       " 0.014690999872982502,\n",
       " -0.03935476392507553,\n",
       " -0.013996644876897335,\n",
       " 0.030431542545557022,\n",
       " 0.03836773335933685,\n",
       " 0.011346074752509594,\n",
       " 0.030062878504395485,\n",
       " 0.005497603677213192,\n",
       " 0.0224442221224308,\n",
       " 0.0380115881562233,\n",
       " 0.09371612221002579,\n",
       " -0.01080279890447855,\n",
       " -0.029513083398342133,\n",
       " -0.033204659819602966,\n",
       " -0.04599704593420029,\n",
       " 0.05811413750052452,\n",
       " -0.019594339653849602,\n",
       " -0.00039688541437499225,\n",
       " -0.0009085030760616064,\n",
       " 0.03361160680651665,\n",
       " -0.057503167539834976,\n",
       " 0.02380257472395897,\n",
       " 0.027473436668515205,\n",
       " -0.024982700124382973,\n",
       " 0.031604018062353134,\n",
       " -0.05084764212369919,\n",
       " 0.035159818828105927,\n",
       " 0.046965405344963074,\n",
       " 0.004097939934581518,\n",
       " 0.01849610172212124,\n",
       " -0.0021117643918842077,\n",
       " 0.015723440796136856,\n",
       " 0.002501982729882002,\n",
       " 0.015250339172780514,\n",
       " -0.051931239664554596,\n",
       " -0.01793062873184681,\n",
       " 0.028043176978826523,\n",
       " 0.001366604003123939,\n",
       " -0.025482535362243652,\n",
       " 0.039024919271469116,\n",
       " -0.030479952692985535,\n",
       " 0.0072034671902656555,\n",
       " -0.01793581061065197,\n",
       " -0.023566121235489845,\n",
       " -0.008717426098883152,\n",
       " -0.07207869738340378,\n",
       " 0.027297623455524445,\n",
       " -0.0149855250492692,\n",
       " -0.0070289927534759045,\n",
       " -0.0036841859109699726,\n",
       " 0.08763180673122406,\n",
       " 0.045950427651405334,\n",
       " 0.03513237088918686,\n",
       " -0.0479729063808918,\n",
       " 0.04637295380234718,\n",
       " -0.13193446397781372,\n",
       " 0.03933511674404144,\n",
       " 0.08731205761432648,\n",
       " 0.00673861475661397,\n",
       " -0.014164158143103123,\n",
       " -0.011414125561714172,\n",
       " -0.01340723317116499,\n",
       " 0.026062265038490295,\n",
       " -0.02805935963988304,\n",
       " 0.00044005687232129276,\n",
       " -0.01990748569369316,\n",
       " -0.033481452614068985,\n",
       " 0.0145639693364501,\n",
       " -0.10082335025072098,\n",
       " -0.08292423188686371,\n",
       " 0.045989740639925,\n",
       " -0.06710654497146606,\n",
       " 0.003087876830250025,\n",
       " -0.07372380048036575,\n",
       " -0.015844164416193962,\n",
       " -0.0016213306225836277,\n",
       " 0.009917707182466984,\n",
       " 0.0035964727867394686,\n",
       " -0.026519538834691048,\n",
       " -0.011823507025837898,\n",
       " 0.03829670697450638,\n",
       " 0.07615848630666733,\n",
       " 0.04306589812040329,\n",
       " -0.0010358832078054547,\n",
       " 0.020498674362897873,\n",
       " 0.0486716665327549,\n",
       " -0.08685220777988434,\n",
       " 0.04283200949430466,\n",
       " -0.023423995822668076,\n",
       " 0.003706936491653323,\n",
       " -0.0204718466848135,\n",
       " -0.04518628492951393,\n",
       " 0.10950269550085068,\n",
       " -0.003565843217074871,\n",
       " -0.011798560619354248,\n",
       " -0.022442977875471115,\n",
       " 0.010161696001887321,\n",
       " 0.07579071074724197,\n",
       " -0.061155401170253754,\n",
       " 0.0001653074286878109,\n",
       " -0.03289943188428879,\n",
       " -0.05590558052062988,\n",
       " -0.03125299885869026,\n",
       " 0.03749895468354225,\n",
       " -0.021048957481980324,\n",
       " 0.02367372065782547,\n",
       " -0.023330794647336006,\n",
       " -0.09069336205720901,\n",
       " -0.005238535348325968,\n",
       " 0.02462979033589363,\n",
       " -0.006404485087841749,\n",
       " -0.06605778634548187,\n",
       " 0.016859689727425575,\n",
       " -0.00781496986746788,\n",
       " -0.04299570992588997,\n",
       " -0.017040856182575226,\n",
       " 0.01206424180418253,\n",
       " -0.02323564514517784,\n",
       " -0.014752049930393696,\n",
       " -0.012376420199871063,\n",
       " -0.019085915759205818,\n",
       " -0.03826259821653366,\n",
       " -0.03853704780340195,\n",
       " -0.004041225649416447,\n",
       " 0.004263586830347776,\n",
       " -0.00562856812030077,\n",
       " 0.0017859144136309624,\n",
       " -0.042410459369421005,\n",
       " -0.019128456711769104,\n",
       " -0.06918618083000183,\n",
       " -0.0034719500690698624,\n",
       " -0.003076647873967886,\n",
       " 0.028295911848545074,\n",
       " 0.06319868564605713,\n",
       " 0.017966389656066895,\n",
       " -0.019468821585178375,\n",
       " 0.09761016815900803,\n",
       " -0.022594448179006577,\n",
       " -0.030214447528123856,\n",
       " -0.006643642671406269,\n",
       " 0.003954456187784672,\n",
       " 0.015823692083358765,\n",
       " -0.0057395584881305695,\n",
       " 0.006218750961124897,\n",
       " 0.1495407521724701,\n",
       " 0.01693890616297722,\n",
       " -0.00821874663233757,\n",
       " 0.0038708129432052374,\n",
       " 0.002723533194512129,\n",
       " 0.013476070016622543,\n",
       " 0.04837753623723984,\n",
       " -0.04417393356561661,\n",
       " 0.0549301840364933,\n",
       " 0.03815314918756485,\n",
       " -0.019431661814451218,\n",
       " -0.05248187482357025,\n",
       " 0.04267038777470589,\n",
       " 0.00018712365999817848,\n",
       " -0.038284216076135635,\n",
       " -0.019774608314037323,\n",
       " 0.0023190639913082123,\n",
       " 0.004980719648301601,\n",
       " -0.021967202425003052,\n",
       " -0.013559315353631973,\n",
       " -0.0761573314666748,\n",
       " -0.007667581085115671,\n",
       " 0.031185604631900787,\n",
       " 0.004409408196806908,\n",
       " -0.015301787294447422,\n",
       " 0.00450381962582469,\n",
       " -0.002307589165866375,\n",
       " -0.04717826470732689,\n",
       " 0.1009579598903656,\n",
       " -0.030725916847586632,\n",
       " -0.021748092025518417,\n",
       " 0.013946682214736938,\n",
       " 0.030516328290104866,\n",
       " 0.16992126405239105,\n",
       " -0.02205481566488743,\n",
       " -0.006287210620939732,\n",
       " -0.039746977388858795,\n",
       " 0.015652233734726906,\n",
       " 0.06709837168455124,\n",
       " 0.02886919677257538,\n",
       " -0.009435592219233513,\n",
       " 0.023423094302415848,\n",
       " -0.051977574825286865,\n",
       " 0.019673680886626244,\n",
       " 0.05225013568997383,\n",
       " 0.012521679513156414,\n",
       " -0.004491325933486223,\n",
       " -0.052162736654281616,\n",
       " 0.08205386251211166,\n",
       " -0.033573757857084274,\n",
       " 0.014552797190845013,\n",
       " 0.05250309407711029,\n",
       " -0.02566111832857132,\n",
       " -0.022235311567783356,\n",
       " -0.005412168800830841,\n",
       " -0.03342817351222038,\n",
       " -0.021084152162075043,\n",
       " -0.007612823974341154,\n",
       " -0.0035248317290097475,\n",
       " 0.05973722040653229,\n",
       " 0.04241156950592995,\n",
       " 0.002531873295083642,\n",
       " 0.0107693737372756,\n",
       " 0.048101067543029785,\n",
       " 0.03420724719762802,\n",
       " 0.0032766140066087246,\n",
       " -0.014917971566319466,\n",
       " -0.04267316311597824,\n",
       " 0.02095058001577854,\n",
       " 0.05429135262966156,\n",
       " -0.07778216898441315,\n",
       " -0.004338940605521202,\n",
       " 0.026628483086824417,\n",
       " 0.03528750315308571,\n",
       " 0.024499664083123207,\n",
       " -0.06562765687704086,\n",
       " -0.021876651793718338,\n",
       " 0.019535033032298088,\n",
       " -0.08593667298555374,\n",
       " -0.018787888810038567,\n",
       " -0.004204754251986742,\n",
       " -0.055769842118024826,\n",
       " 0.04233839362859726,\n",
       " -0.016881560906767845,\n",
       " 0.027332816272974014,\n",
       " -0.029894357547163963,\n",
       " 0.018307650461792946,\n",
       " 0.003748994553461671,\n",
       " 0.018004300072789192,\n",
       " 0.03947386518120766,\n",
       " 0.030010048300027847,\n",
       " -0.019976617768406868,\n",
       " -0.048508547246456146,\n",
       " -0.011991719715297222,\n",
       " 0.01288251206278801,\n",
       " -0.0041977353394031525,\n",
       " 0.008116820827126503,\n",
       " -0.045485008507966995,\n",
       " 0.030399460345506668,\n",
       " 0.015875864773988724,\n",
       " 0.03053710050880909,\n",
       " -0.004972073715180159,\n",
       " 0.015153086744248867,\n",
       " 0.06365793943405151,\n",
       " 0.0016650225734338164,\n",
       " -0.047125138342380524,\n",
       " 0.027310995385050774,\n",
       " 0.0699881762266159,\n",
       " 0.06163886934518814,\n",
       " 0.01628776825964451,\n",
       " -0.021954279392957687,\n",
       " -0.03765365108847618,\n",
       " 0.04707128554582596,\n",
       " 0.08143194764852524,\n",
       " 0.04646435007452965,\n",
       " -0.08307783305644989,\n",
       " -0.028530582785606384,\n",
       " -0.02347893826663494,\n",
       " -0.01547681912779808,\n",
       " 0.025934087112545967,\n",
       " 0.010208827443420887,\n",
       " 0.001996210077777505,\n",
       " -0.03330458328127861,\n",
       " 0.022766146808862686,\n",
       " 0.03556124493479729,\n",
       " -0.06583463400602341,\n",
       " -0.02399711310863495,\n",
       " -0.004286924377083778,\n",
       " -0.03751301020383835,\n",
       " -0.0094144893810153,\n",
       " -0.039430487900972366,\n",
       " -0.06661071628332138,\n",
       " 0.06391461193561554,\n",
       " -0.012115426361560822,\n",
       " -0.06977066397666931,\n",
       " 0.004653838463127613,\n",
       " 0.023481085896492004,\n",
       " 0.002979827579110861,\n",
       " -0.0019254098879173398,\n",
       " -0.008050278760492802,\n",
       " -0.05609937384724617,\n",
       " 0.007191741839051247,\n",
       " -0.02680736593902111,\n",
       " -0.03452039137482643,\n",
       " -0.08814822137355804,\n",
       " -0.0663471445441246,\n",
       " -0.006973163690418005,\n",
       " 0.03777505084872246,\n",
       " 0.02242966741323471,\n",
       " -0.031578533351421356,\n",
       " -0.05772729963064194,\n",
       " 0.07263421267271042,\n",
       " -0.07842699438333511,\n",
       " 0.030790647491812706,\n",
       " -0.03632966801524162,\n",
       " -0.015109667554497719,\n",
       " -0.021638287231326103,\n",
       " 0.011402691714465618,\n",
       " 0.03130708262324333,\n",
       " 0.02856660820543766,\n",
       " -0.021631112322211266,\n",
       " 0.03675498813390732,\n",
       " 0.002767953323200345,\n",
       " -0.08999279886484146,\n",
       " -0.002105662366375327,\n",
       " 0.027219722047448158,\n",
       " -0.09420477598905563,\n",
       " 0.014551057480275631,\n",
       " 0.027269240468740463,\n",
       " -0.013652779161930084,\n",
       " -0.08116953074932098,\n",
       " 0.0591990202665329,\n",
       " 0.007549918256700039,\n",
       " 0.07088397443294525,\n",
       " -0.035698167979717255,\n",
       " -0.02225809171795845,\n",
       " 0.05049655959010124,\n",
       " 0.06769665330648422,\n",
       " 0.03625679388642311,\n",
       " -0.02409343048930168,\n",
       " -0.006131120026111603,\n",
       " 0.022618211805820465,\n",
       " 0.026057925075292587,\n",
       " 0.0037241997197270393,\n",
       " 0.012332930229604244,\n",
       " -0.07911744713783264,\n",
       " 0.04956492781639099,\n",
       " 0.03544715791940689,\n",
       " 0.05787383392453194,\n",
       " 0.002236581640318036,\n",
       " 0.09269050508737564,\n",
       " -0.006648804061114788,\n",
       " -0.010257339105010033,\n",
       " -0.08847735077142715,\n",
       " -0.019082721322774887,\n",
       " -0.017151961103081703,\n",
       " 0.019423533231019974,\n",
       " 0.017143912613391876,\n",
       " -0.023540833964943886,\n",
       " -0.02161034382879734,\n",
       " -0.043320976197719574,\n",
       " -0.006400718819350004,\n",
       " -0.029040373861789703,\n",
       " -0.00832358468323946,\n",
       " -0.03415295481681824,\n",
       " 0.10155341774225235,\n",
       " -0.006426061503589153,\n",
       " -0.016629468649625778,\n",
       " 0.004610372707247734,\n",
       " 0.033164188265800476,\n",
       " 0.05571998283267021,\n",
       " -0.02814774587750435,\n",
       " -0.015214437618851662,\n",
       " -0.0673823356628418,\n",
       " 0.008494673296809196,\n",
       " -0.022199641913175583,\n",
       " 0.0064285085536539555,\n",
       " -0.018255257979035378,\n",
       " 0.020322872325778008,\n",
       " 0.05334171652793884,\n",
       " -0.012593818828463554,\n",
       " 0.004384810104966164,\n",
       " 0.004278929438441992,\n",
       " -0.030798938125371933,\n",
       " 0.03964060917496681,\n",
       " 0.02182644046843052,\n",
       " 0.037160828709602356,\n",
       " 0.018981944769620895,\n",
       " 0.010348130948841572,\n",
       " -0.05694608390331268,\n",
       " 0.016682421788573265,\n",
       " -0.01800362765789032,\n",
       " 0.01118103414773941,\n",
       " -0.03338001295924187,\n",
       " -0.008563670329749584,\n",
       " -0.0066829705610871315,\n",
       " 0.033253856003284454,\n",
       " 0.016633756458759308,\n",
       " -0.024899134412407875,\n",
       " 0.018370427191257477,\n",
       " -0.07499856501817703,\n",
       " -0.006183937657624483,\n",
       " -0.05371396616101265,\n",
       " 0.015435449779033661,\n",
       " -0.028503302484750748,\n",
       " -0.005056907422840595,\n",
       " 0.017611606046557426,\n",
       " -0.01341059897094965,\n",
       " -0.039678748697042465,\n",
       " -0.009593864902853966,\n",
       " 0.0150945158675313,\n",
       " -0.04081335663795471,\n",
       " -0.021303806453943253,\n",
       " -0.054372381418943405,\n",
       " 0.07993888109922409,\n",
       " 0.08850128203630447,\n",
       " 0.05530255660414696,\n",
       " -0.04733571037650108,\n",
       " 0.005534918513149023,\n",
       " -0.04609394818544388,\n",
       " 0.022199856117367744,\n",
       " 0.022407976910471916,\n",
       " 0.07335735112428665,\n",
       " 0.02088792994618416,\n",
       " 0.023396385833621025,\n",
       " 0.10898000001907349,\n",
       " 0.0016419283347204328,\n",
       " 0.014158010482788086,\n",
       " 0.012128693051636219,\n",
       " 0.009659511968493462,\n",
       " -0.029435433447360992,\n",
       " -0.06548787653446198,\n",
       " -0.020077424123883247,\n",
       " 0.006858943495899439,\n",
       " -0.062253061681985855,\n",
       " 0.06189621239900589,\n",
       " -0.08477386832237244,\n",
       " -0.021575545892119408,\n",
       " -0.05323116481304169,\n",
       " 0.03685729205608368,\n",
       " -0.012970931828022003,\n",
       " 0.06511634588241577,\n",
       " 0.018723536282777786,\n",
       " -0.07416938990354538,\n",
       " 0.08330590277910233,\n",
       " -0.07045262306928635,\n",
       " -0.00755632808431983,\n",
       " -0.013182064518332481,\n",
       " -0.02179666981101036,\n",
       " -0.01747845858335495,\n",
       " 0.0020103766582906246,\n",
       " 0.02620396763086319,\n",
       " -0.037039436399936676,\n",
       " 0.036238931119441986,\n",
       " 0.027578238397836685,\n",
       " -0.06718024611473083,\n",
       " -0.015645192936062813,\n",
       " -0.02371453307569027,\n",
       " 0.026532255113124847,\n",
       " -0.027724403887987137,\n",
       " -0.011156551539897919,\n",
       " -0.0235466118901968,\n",
       " 0.002830840414389968,\n",
       " -0.018597230315208435,\n",
       " -0.02523050270974636,\n",
       " 0.04777301102876663,\n",
       " -0.0626966655254364,\n",
       " -0.027037905529141426,\n",
       " 0.0037979446351528168,\n",
       " -0.043596889823675156,\n",
       " -0.014589954167604446,\n",
       " -0.0019399201264604926,\n",
       " -0.11221926659345627,\n",
       " -0.04031376540660858,\n",
       " -0.0027294682804495096,\n",
       " 0.015116171911358833,\n",
       " 0.0026236772537231445,\n",
       " 0.006981036625802517,\n",
       " -0.07040311396121979,\n",
       " -0.04969130456447601,\n",
       " -0.004407715518027544,\n",
       " 0.013367991894483566,\n",
       " -0.024045446887612343,\n",
       " -0.042442258447408676,\n",
       " -0.007622863631695509,\n",
       " -0.07525208592414856,\n",
       " 0.018859928473830223,\n",
       " -0.02515607327222824,\n",
       " 0.009357892908155918,\n",
       " 0.054626040160655975,\n",
       " 0.0472027026116848,\n",
       " 0.021603405475616455,\n",
       " -0.012970488518476486,\n",
       " -0.04241316393017769,\n",
       " 0.0014785168459638953,\n",
       " -0.05326460674405098,\n",
       " 0.000361678161425516,\n",
       " -0.013640141114592552,\n",
       " 0.04973476380109787,\n",
       " -0.008382989093661308,\n",
       " 0.006717284210026264,\n",
       " 0.04056905210018158]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "with open('submission.csv', 'w', newline='\\n') as submit_file:\n",
    "    with open('inference.csv', 'r') as inference_file:\n",
    "        infer_reader = csv.reader(inference_file)\n",
    "        submit_writer = csv.writer(submit_file)\n",
    "\n",
    "        submit_writer.writerow(['id'] + ['vec%d' % i for i in range(512)])\n",
    "        next(infer_reader)\n",
    "\n",
    "        for id, answer in infer_reader:\n",
    "            vector = embedding_model.encode(answer)\n",
    "            submit_writer.writerow([id] + vector.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upstage-finetuning-pvueOKD_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
